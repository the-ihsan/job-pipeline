# Job Pipeline

A tiny TypeScript job runner and composable pipeline for scraping/ETL with Puppeteer and optional AI (Gemini).

## ğŸ¯ Features

- **CLI job runner**: `pnpm job <jobName>` or interactive picker
- **Composable pipeline**: `start(init, state).pipe(...).saveAs(...).run()`
- **Built-in CSV/JSON/TXT output** stored per-job under `jobs/<job>/output/`
- **Gemini client + rate limiting** in `utils/ai.ts` (optional)

## ğŸ“¦ Requirements

- Node.js â‰¥ 22 (uses `--experimental-strip-types`)
- pnpm â‰¥ 8

## ğŸš€ Quick start

```bash
pnpm install

# Run a job directly
pnpm job newspaper1

# Or pick interactively (shows a numbered list)
pnpm job
```

The CLI script is `cli.ts`. When a job runs, the environment variable `JOB_NAME` is set automatically and outputs are written under `jobs/$JOB_NAME/output/`.

## ğŸ§° Available jobs

- **newspaper1**: Crawl `allonlinebanglanewspapers.com` to collect newspaper names and details.
  - Saves: `jobs/newspaper1/output/newspaper-d.json`
  - Extracts emails with preferences and saves: `jobs/newspaper1/output/newspaper-with-email-d.csv`
- **search-helper**: Opens Google queries for each scraped name to manually find emails; waits for Enter to continue. Does not write output files.
- **filter-uniques**: De-duplicates rows by `Email` from `jobs/filter-uniques/data.csv` and saves `jobs/filter-uniques/output/filtered.csv`.

## ğŸ”§ Scripts

- `pnpm job` â€” run the CLI (`node --experimental-strip-types cli.ts`)
- `pnpm lint` â€” TypeScript typecheck only (`tsc --noEmit`)

## ğŸ—‚ï¸ Project layout

```
rough/
â”œâ”€â”€ cli.ts                  # Job runner (argument or interactive)
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ newspaper1/         # Scraper that saves JSON + CSV
â”‚   â”‚   â”œâ”€â”€ index.ts
â”‚   â”‚   â”œâ”€â”€ utils.ts
â”‚   â”‚   â””â”€â”€ output/
â”‚   â”œâ”€â”€ search-helper/      # Manual assist flow (opens searches)
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â””â”€â”€ filter-uniques/     # CSV de-dup by Email
â”‚       â”œâ”€â”€ data.csv
â”‚       â””â”€â”€ index.ts
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ ai.ts               # Gemini client + rate limiter
â”‚   â”œâ”€â”€ file.ts             # saveToJSON/CSV/TXT, loadCSV/JSON
â”‚   â”œâ”€â”€ job.ts              # Pipeline implementation
â”‚   â””â”€â”€ index.ts            # Public exports + start(), waitForInput()
â””â”€â”€ tsconfig.json
```

## ğŸ§ª Using the pipeline

Minimal example of a job using the pipeline:

```ts
// jobs/my-job/index.ts
import { start } from '../../utils/index.ts';

interface JobState {
  counter: number;
}

const init = async ({ counter }: JobState) => {
  return Array.from({ length: counter }, (_, i) => i + 1);
};

const squareAll = async (nums: number[]) => nums.map(n => n * n);

export default async function main() {
  await start<JobState>(init, { counter: 5 })
    .pipe(squareAll)
    .saveAs('squares.json') // -> jobs/my-job/output/squares.json
    .run();
}
```

Key helpers:

- `pipe(fn)`: pass whole data through a step
- `pipeSliced(fn, size)`: call a step on array slices (batched)
- `pipeEach(fn)`: call a step per item, collect results
- `pipeEachFiltered(fn)`: like `pipeEach` but drops falsy returns
- `saveAs(filename)`: auto-saves into `jobs/$JOB_NAME/output/`

## ğŸ”‘ Environment variables

- `GEMINI_API_KEY` (optional): required only if your job uses `utils/ai.ts`.
  - Loaded via `dotenv` if present in a `.env` file at repo root.

## ğŸ“ Notes

- Puppeteer jobs run headless and include Linux-friendly flags (no-sandbox, etc.).
- Outputs are created lazily; directories are made if missing.

## ğŸ“„ License

MIT

